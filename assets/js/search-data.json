{
  
    
        "post0": {
            "title": "Streaming voice activity detection with pyannote.audio",
            "content": "In this blog post, I describe an early attempt at performing live voice activity detection with pyannote.audio pretrained segmentation model. . Requirements . install pyannote.audio from the develop branch | install streamz | . # setting up for pretty visualization %matplotlib inline import matplotlib.pyplot as plt from pyannote.core import notebook, Segment, SlidingWindow from pyannote.core import SlidingWindowFeature as SWF notebook.crop = Segment(0, 10) def visualize(features): figsize = plt.rcParams[&quot;figure.figsize&quot;] plt.rcParams[&quot;figure.figsize&quot;] = (notebook.width, 2) notebook.plot_feature(features) . . Rolling audio buffer . Let us assume that the audio stream is given as a 5s rolling buffer. Here, we are going to fake it by sliding a 5s window over the duration of an audio file. . from pyannote.audio.core.io import Audio, AudioFile class RollingAudioBuffer(Audio): &quot;&quot;&quot;Rolling audio buffer Parameters - sample_rate : int Sample rate duration : float, optional Duration of rolling buffer. Defaults to 5s. step : float, optional Delay between two updates of the rolling buffer. Defaults to 1s. Usage -- &gt;&gt;&gt; buffer = RollingAudioBuffer()(&quot;audio.wav&quot;) &gt;&gt;&gt; current_buffer = next(buffer) &quot;&quot;&quot; def __init__(self, sample_rate=16000, duration=5.0, step=1.): super().__init__(sample_rate=sample_rate, mono=True) self.duration = duration self.step = step def __call__(self, file: AudioFile): # duration of the whole audio file duration = self.get_duration(file) # slide a 5s window from the beginning to the end of the file window = SlidingWindow(start=0., duration=self.duration, step=self.step, end=duration) for chunk in window: # for each position of the window, yield the corresponding audio buffer # as a SlidingWindowFeature instance waveform, sample_rate = self.crop(file, chunk, duration=self.duration) resolution = SlidingWindow(start=chunk.start, duration=1./self.sample_rate, step=1./sample_rate) yield SWF(waveform.T, resolution) . /Users/bredin/miniconda3/envs/pyannote/lib/python3.8/site-packages/torchaudio/backend/utils.py:46: UserWarning: &#34;torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE&#34; flag is deprecated and will be removed in 0.9.0. Please remove the use of flag. warnings.warn( . . We start by initializing rolling buffer on a sample file: . MY_AUDIO_FILE = &quot;DH_0001.flac&quot; buffer = RollingAudioBuffer()(MY_AUDIO_FILE) . Each subsequent call to next(buffer) returns the current content of the 5s rolling buffer: . next(buffer) . next(buffer) . next(buffer) . For illustration purposes, we also load the manual voice activity reference. . from pyannote.database.util import load_rttm reference = load_rttm(&#39;DH_0001.rttm&#39;).popitem()[1].get_timeline() reference . Pretrained voice activity detection model . pyannote.audio comes with a decent pretrained segmentation model that can be used for voice activity detection. . import torch import numpy as np from pyannote.audio import Model class VoiceActivityDetection: def __init__(self): self.model = Model.from_pretrained(&quot;pyannote/segmentation&quot;) self.model.eval() def __call__(self, current_buffer: SWF) -&gt; SWF: # we start by applying the model on the current buffer with torch.no_grad(): waveform = current_buffer.data.T segmentation = self.model(waveform[np.newaxis]).numpy()[0] # temporal resolution of the output of the model resolution = self.model.introspection.frames # temporal shift to keep track of current buffer start time resolution = SlidingWindow(start=current_buffer.sliding_window.start, duration=resolution.duration, step=resolution.step) # pyannote/segmentation pretrained model actually does more than just voice activity detection # see https://huggingface.co/pyannote/segmentation for more details. speech_probability = np.max(segmentation, axis=-1, keepdims=True) return SWF(speech_probability, resolution) . vad = VoiceActivityDetection() . Let us try this thing on current buffer: . current_buffer = next(buffer) current_buffer . vad(current_buffer) . reference . Building a basic streaming pipeline with streamz . We now have a way to stream audio and apply voice activity detection. According to its documentation, streamz seems like a good option to do that: . Streamz helps you build pipelines to manage continuous streams of data. . Let us start by creating a Stream that will ingest the rolling buffer and apply voice activity detection anytime the buffer is updated. . from streamz import Stream source = Stream() source.map(vad).sink(visualize) . We re-initialize the audio buffer from the start of the file and push the rolling buffer into the pipeline: . buffer = RollingAudioBuffer()(MY_AUDIO_FILE) source.emit(next(buffer)) . source.emit(next(buffer)) . source.emit(next(buffer)) . reference . Controlling latency / accuracy trade-off . This is nice but we can do better in case the pipeline is allowed a small delay (a.k.a. latency) between when it receives the audio and when it outputs the voice activity detection scores. . For instance, if we are allowed 2s latency, we could benefit from the multiple overlapping buffers and combine them to get a better estimate of the speech probability in regions where the model is not quite confident (e.g. just before t=4s). . This is what the Aggregation class does. . from typing import Tuple, List class Aggregation: &quot;&quot;&quot;Aggregate multiple overlapping buffers with a Parameters - latency : float, optional Allowed latency, in seconds. Defaults to 0. &quot;&quot;&quot; def __init__(self, latency=0.0): self.latency = latency def __call__(self, internal_state, current_buffer: SWF) -&gt; Tuple[Tuple[float, List[SWF]], SWF]: &quot;&quot;&quot;Ingest new buffer and return aggregated output with delay Parameters - internal_state : (internal_time, past_buffers) tuple `internal_time` is a float such that previous call emitted aggregated scores up to time `delayed_time`. `past_buffers` is a rolling list of past buffers that we are going to aggregate. current_buffer : SlidingWindowFeature New incoming score buffer. &quot;&quot;&quot; if internal_state is None: internal_state = (0.0, list()) # previous call led to the emission of aggregated scores up to time `delayed_time` # `past_buffers` is a rolling list of past buffers that we are going to aggregate delayed_time, past_buffers = internal_state # real time is the current end time of the audio buffer # (here, estimated from the end time of the VAD buffer) real_time = current_buffer.extent.end # because we are only allowed `self.latency` seconds of latency, this call should # return aggregated scores for [delayed_time, real_time - latency] time range. required = Segment(delayed_time, real_time - self.latency) # to compute more robust scores, we will combine all buffers that have a non-empty # temporal intersection with required time range. we can get rid of the others as they # will no longer be needed as they are too far away in the past. past_buffers = [buffer for buffer in past_buffers if buffer.extent.end &gt; required.start] + [current_buffer] # we aggregate all past buffers (but only on the &#39;required&#39; region of interest) intersection = np.stack([buffer.crop(required, fixed=required.duration) for buffer in past_buffers]) aggregation = np.mean(intersection, axis=0) # ... and wrap it into a self-contained SlidingWindowFeature (SWF) instance resolution = current_buffer.sliding_window resolution = SlidingWindow(start=required.start, duration=resolution.duration, step=resolution.step) output = SWF(aggregation, resolution) # we update the internal state delayed_time = real_time - self.latency internal_state = (delayed_time, past_buffers) # ... and return the whole thing for next call to know where we are return internal_state, output . Let&#39;s add this new accumulator into the streaming pipeline, with a 2s latency: . source = Stream() source .map(vad) .accumulate(Aggregation(latency=2.), returns_state=True, start=None) .sink(visualize) buffer = RollingAudioBuffer()(MY_AUDIO_FILE) . current_buffer = next(buffer); current_buffer . source.emit(current_buffer) . current_buffer = next(buffer); current_buffer . source.emit(current_buffer) . current_buffer = next(buffer); current_buffer . source.emit(current_buffer) . Look how the aggregation process actually refined the speech probability just before t=4s. This has been enabled by the longer latency. . That&#39;s all folks! . For technical questions and bug reports, please check pyannote.audio Github repository. . For commercial enquiries and scientific consulting, please contact me. . Bonus: concatenating output . For visualization purposes, you might want to add an accumulator to the pipeline that takes care of concatenating the output of each step... . class Concatenation: def __call__(self, concatenation: SWF, current_buffer: SWF) -&gt; Tuple[SWF, SWF]: if concatenation is None: return current_buffer, current_buffer resolution = concatenation.sliding_window current_start_frame = resolution.closest_frame(current_buffer.extent.start) current_end_frame = current_start_frame + len(current_buffer) concatenation.data = np.pad(concatenation.data, ((0, current_end_frame - len(concatenation.data)), (0, 0))) concatenation.data[current_start_frame: current_end_frame] = current_buffer.data return concatenation, concatenation . source = Stream() source .map(vad) .accumulate(Aggregation(latency=2.), returns_state=True, start=None) .accumulate(Concatenation(), returns_state=True, start=None) .sink(visualize) buffer = RollingAudioBuffer()(MY_AUDIO_FILE) . notebook.crop = Segment(0, 30) for _ in range(30): source.emit(next(buffer)) . reference .",
            "url": "https://herve.niderb.fr/fastpages/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html",
            "relUrl": "/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html",
            "date": " • Aug 5, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "Biography . I received my PhD on talking-face biometric authentication from Telecom ParisTech (France) in 2007. . In 2008, I joined Dublin City University (Ireland) to work on video summarization techniques applied to user-generated content. . I have been a permanent CNRS researcher since 2008: . at IRIT (Toulouse, France) in the SAMoVA group until 2010 | at LIMSI (Orsay, France) in the Spoken Language Processing group until 2020 | back at IRIT (Toulouse, France) until today | . My current research interests include deep learning techniques applied to machine listening, with a strong focus on speaker diarization: I am proud to be known as “the guy behind pyannote“. . Since 2021, I have also started working as a self-employed scientific advisor and am available for contracting on topics related to my academic research. . Contact . Postal Address: IRIT, 118 Route de Narbonne, F-31062 Toulouse, Cedex, France | Office: IRIT1 219 (2nd floor) | E-mail: herve.bredin@irit.fr | Tel: +33 (0)5 61 55 68 86 | .",
          "url": "https://herve.niderb.fr/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Collaborators",
          "content": "Current PhD students . Marvin Lavechin (2019 - today) . Unsupervised and multimodal protolexicon discovery | Co-supervised with Alejandrina Cristia (LSCP) and Emmanuel Dupoux (LSCP, FAIR) | . Juan Manuel Coria (2019 - today) . Active representation learning | Co-supervised with Sophie Rosset (LISN) and Sahar Ghannay (LISN) | . Léo Galmant (2018 - today) . Machine learning for the automatic structuring of spoken conversations | Co-supervised with Anne-Laure Ligozat (LISN) and Camille Guinaudeau (LISN) | . Former PhD students . Ruiqing Yin (2016-2019) . Steps towards end-to-end neural speaker diarization | Co-supervised with Claude Barras (LIMSI) | Now - Research scientist at Huawei | . Philipe Ercolessi (2010-2013) . Multimodal extraction of the narrative structure of TV series episodes | Co-supervised with Philippe Joly (IRIT) and Christine Sénac (IRIT) | Now - Head of research at Admo.tv | .",
          "url": "https://herve.niderb.fr/fastpages/collab/",
          "relUrl": "/collab/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "",
          "content": "Academic researcher . Deep learning for audio, speech and natural language processing . Tenured research scientist at French national institute for scientific research (CNRS) | Member of the Institut de Recherche en Informatique de Toulouse (IRIT) | Member of the SAMoVA team | . Publications: HAL or Google Scholar . Scientific advisor . Self-employed, available for contracting . Machine learning | Deep learning | Speech processing | Natural language processing | Machine listening | . Open source developer . Reproducible research advocate . Lead developer of pyannote.audio a deep learning toolkit for speaker diarization | Lead developer of pyannote.video face detection, tracking and clustering in videos | Lead developer of pyannote.metrics a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems | . All projects: Github . Blog .",
          "url": "https://herve.niderb.fr/fastpages/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://herve.niderb.fr/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}