{
  
    
        "post0": {
            "title": "One speaker segmentation model to rule them all",
            "content": "# setting things for pretty visualization from rich import print from pyannote.core import notebook, Segment SAMPLE_EXTENT = Segment(0, 30) notebook.crop = SAMPLE_EXTENT SAMPLE_CHUNK = Segment(15, 20) SAMPLE_URI = &quot;sample&quot; SAMPLE_WAV = f&quot;{SAMPLE_URI}.wav&quot; SAMPLE_REF = f&quot;{SAMPLE_URI}.rttm&quot; . . In this blog post, I talk about pyannote.audio pretrained speaker segmentation model, which happens to be one of the most popular audio model available on 🤗 Huggingface model hub. . from pyannote.audio import Model model = Model.from_pretrained(&quot;pyannote/segmentation&quot;) . What does pyannote/segmentation do? . Every pyannote.audio model has a specifications attribute that tells us a bit more about itself: . print(model.specifications) . Specifications( problem=&lt;Problem.MULTI_LABEL_CLASSIFICATION: 2&gt;, resolution=&lt;Resolution.FRAME: 1&gt;, duration=5.0, warm_up=(0.0, 0.0), classes=[&#39;speaker#1&#39;, &#39;speaker#2&#39;, &#39;speaker#3&#39;], permutation_invariant=True ) . These specifications tell us the following about pyannote/segmentation: . it ingests audio chunks of 5 seconds duration | it addresses a multi-label classification problem... | ... whose possible classes are chosen among speaker#1, speaker#2, and speaker#3 ... | ... and are permutation_invariant (more about that below) | . We also learn that its output temporal resolution is the frame (i.e. it outputs a sequence of frame-wise decisions rather than just one decision for the whole chunk). The actual temporal resolution can be obtained through the magic introspection attribute (approximately 17ms for pyannote/segmentation): . model.introspection.frames.step . 0.016875 . OK, but what does pyannote/segmentation really do? . To answer this question, let us consider the audio recording of a 30s conversation between two speakers (the blue one and the red one): . from pyannote.database.util import load_rttm reference = load_rttm(SAMPLE_REF)[SAMPLE_URI] reference . . from IPython.display import Audio as AudioPlayer AudioPlayer(SAMPLE_WAV) . . Your browser does not support the audio element. Let&#39;s apply the model on this 5s excerpt of the conversation: . SAMPLE_CHUNK . . from pyannote.audio import Audio audio_reader = Audio(sample_rate=model.hparams.sample_rate) waveform, sample_rate = audio_reader.crop(SAMPLE_WAV, SAMPLE_CHUNK) . import numpy as np from pyannote.core import SlidingWindowFeature, SlidingWindow _waveform, sample_rate = Audio()(SAMPLE_WAV) _waveform = _waveform.numpy().T _waveform[:round(SAMPLE_CHUNK.start * sample_rate)] = np.NAN _waveform[round(SAMPLE_CHUNK.end * sample_rate):] = np.NAN SlidingWindowFeature(_waveform, SlidingWindow(1./sample_rate, 1./sample_rate)) . . output = model(waveform) . _output = output.detach()[0].numpy() shifted_frames = SlidingWindow(start=SAMPLE_CHUNK.start, duration=model.introspection.frames.duration, step=model.introspection.frames.step) _output = SlidingWindowFeature(_output, shifted_frames) _output . . The model has accurately detected that two speakers are active (the orange one and the blue one) in this 5s chunk, and that they are partially overlapping around t=18s. The third speaker probability (in green) is close to zero for the whole five seconds. . reference.crop(SAMPLE_CHUNK) . . pyannote.audio provides a convenient Inference class to apply the model using a 5s sliding window on the whole file: . from pyannote.audio import Inference inference = Inference(model, duration=5.0, step=2.5) output = inference(SAMPLE_WAV) . output . . output.data.shape . (11, 293, 3) . BATCH_AXIS = 0 TIME_AXIS = 1 SPEAKER_AXIS = 2 . For each of the 11 positions of the 5s window, the model outputs a 3-dimensional vector every 16ms (293 frames for 5 seconds), corresponding to the probabilities that each of (up to) 3 speakers is active. . &quot;He who can do more can do less&quot; . This model has more than one string to its bow and can prove useful for lots of speaker-related tasks such as: . voice activity detection | overlapped speech detection | instantaneous speaker counting | speaker change detection | . Voice activity detection (VAD) . Voice activity detection is the task of detecting speech regions in a given audio stream or recording. This can be achieved by postprocessing the output of the model using the maximum over the speaker axis for each frame. . to_vad = lambda o: np.max(o, axis=SPEAKER_AXIS, keepdims=True) to_vad(output) . The Inference class has a built-in mechanism to apply this transformation automatically on each window and aggregate the result using overlap-add. This is achieved by passing the above to_vad function via the pre_aggregation_hook parameter: . vad = Inference(&quot;pyannote/segmentation&quot;, pre_aggregation_hook=to_vad) vad_prob = vad(SAMPLE_WAV) . vad_prob.labels = [&#39;SPEECH&#39;] vad_prob . . Binarize utility class can eventually convert this frame-based probabiliy to the time domain: . from pyannote.audio.utils.signal import Binarize binarize = Binarize(onset=0.5) speech = binarize(vad_prob) . speech . . reference . . Overlapped speech detection (OSD) . Overlapped speech detection is the task of detecting regions where at least two speakers are speaking at the same time. This can be achieved by postprocessing the output of the model using the second maximum over the speaker axis for each frame. . to_osd = lambda o: np.partition(o, -2, axis=SPEAKER_AXIS)[:, :, -2, np.newaxis] osd = Inference(&quot;pyannote/segmentation&quot;, pre_aggregation_hook=to_osd) osd_prob = osd(SAMPLE_WAV) . osd_prob.labels = [&#39;OVERLAP&#39;] osd_prob . . binarize(osd_prob) . reference . . Instantaneous speaker counting (CNT) . Instantaneous speaker counting is a generalization of voice activity and overlapped speech detection that aims at returning the number of simultaneous speakers at each frame. This can be achieved by summing the output of the model over the speaker axis for each frame: . to_cnt = lambda probability: np.sum(probability, axis=SPEAKER_AXIS, keepdims=True) cnt = Inference(&quot;pyannote/segmentation&quot;, pre_aggregation_hook=to_cnt) cnt(SAMPLE_WAV) . Speaker change detection (SCD) . Speaker change detection is the task of detecting speaker change points in a given audio stream or recording. It can be achieved by taking the absolute value of the first derivative over the time axis, and take the maximum value over the speaker axis: . to_scd = lambda probability: np.max( np.abs(np.diff(probability, n=1, axis=TIME_AXIS)), axis=SPEAKER_AXIS, keepdims=True) scd = Inference(&quot;pyannote/segmentation&quot;, pre_aggregation_hook=to_scd) scd_prob = scd(SAMPLE_WAV) . scd_prob.labels = [&#39;SPEAKER_CHANGE&#39;] scd_prob . . Using a combination of Peak utility class (to detect peaks in the above curve) and voice activity detection output, we can obtain a decent segmentation into speaker turns: . from pyannote.audio.utils.signal import Peak peak = Peak(alpha=0.05) peak(scd_prob).crop(speech.get_timeline()) . reference . . What pyannote/segmentation CANNOT do . Now, let&#39;s take a few steps back and have a closer look at the raw output of the model. . output . . Did you notice that the blue and orange speakers have been swapped between overlapping windows [15s, 20s] and [17.5s, 22.5s]? . This is a (deliberate) side effect of the permutation-invariant training process using when training the model. The model is trained to discriminate speakers locally (i.e. within each window) but does not care about their global identity (i.e. at conversation scale). Have a look at this paper to learn more about this permutation-invariant training thing. . That means that this kind of model does not actually perform speaker diarization out of the box. Luckily, pyannote.audio has got you covered! pyannote/speaker-diarization pretrained pipeline uses pyannote/segmentation internally and combines it with speaker embeddings to deal with this permutation problem globally. . That&#39;s all folks! . For technical questions and bug reports, please check pyannote.audio Github repository. . I also offer scientific consulting services around speaker diarization (and speech processing in general), please contact me if you think this type of technology might help your business/startup! .",
            "url": "https://herve.niderb.fr/fastpages/2022/10/23/One-speaker-segmentation-model-to-rule-them-all.html",
            "relUrl": "/2022/10/23/One-speaker-segmentation-model-to-rule-them-all.html",
            "date": " • Oct 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Streaming voice activity detection with pyannote.audio",
            "content": "In this blog post, I describe an early attempt at performing live voice activity detection with pyannote.audio pretrained segmentation model. . Requirements . install pyannote.audio from the develop branch | install streamz | . # setting up for pretty visualization %matplotlib inline import matplotlib.pyplot as plt from pyannote.core import notebook, Segment, SlidingWindow from pyannote.core import SlidingWindowFeature as SWF notebook.crop = Segment(0, 10) def visualize(features): figsize = plt.rcParams[&quot;figure.figsize&quot;] plt.rcParams[&quot;figure.figsize&quot;] = (notebook.width, 2) notebook.plot_feature(features) . . Rolling audio buffer . Let us assume that the audio stream is given as a 5s rolling buffer. Here, we are going to fake it by sliding a 5s window over the duration of an audio file. . from pyannote.audio.core.io import Audio, AudioFile class RollingAudioBuffer(Audio): &quot;&quot;&quot;Rolling audio buffer Parameters - sample_rate : int Sample rate duration : float, optional Duration of rolling buffer. Defaults to 5s. step : float, optional Delay between two updates of the rolling buffer. Defaults to 1s. Usage -- &gt;&gt;&gt; buffer = RollingAudioBuffer()(&quot;audio.wav&quot;) &gt;&gt;&gt; current_buffer = next(buffer) &quot;&quot;&quot; def __init__(self, sample_rate=16000, duration=5.0, step=1.): super().__init__(sample_rate=sample_rate, mono=True) self.duration = duration self.step = step def __call__(self, file: AudioFile): # duration of the whole audio file duration = self.get_duration(file) # slide a 5s window from the beginning to the end of the file window = SlidingWindow(start=0., duration=self.duration, step=self.step, end=duration) for chunk in window: # for each position of the window, yield the corresponding audio buffer # as a SlidingWindowFeature instance waveform, sample_rate = self.crop(file, chunk, duration=self.duration) resolution = SlidingWindow(start=chunk.start, duration=1./self.sample_rate, step=1./sample_rate) yield SWF(waveform.T, resolution) . /Users/bredin/miniconda3/envs/pyannote/lib/python3.8/site-packages/torchaudio/backend/utils.py:46: UserWarning: &#34;torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE&#34; flag is deprecated and will be removed in 0.9.0. Please remove the use of flag. warnings.warn( . . We start by initializing rolling buffer on a sample file: . MY_AUDIO_FILE = &quot;DH_0001.flac&quot; buffer = RollingAudioBuffer()(MY_AUDIO_FILE) . Each subsequent call to next(buffer) returns the current content of the 5s rolling buffer: . next(buffer) . next(buffer) . next(buffer) . For illustration purposes, we also load the manual voice activity reference. . from pyannote.database.util import load_rttm reference = load_rttm(&#39;DH_0001.rttm&#39;).popitem()[1].get_timeline() reference . Pretrained voice activity detection model . pyannote.audio comes with a decent pretrained segmentation model that can be used for voice activity detection. . import torch import numpy as np from pyannote.audio import Model class VoiceActivityDetection: def __init__(self): self.model = Model.from_pretrained(&quot;pyannote/segmentation&quot;) self.model.eval() def __call__(self, current_buffer: SWF) -&gt; SWF: # we start by applying the model on the current buffer with torch.no_grad(): waveform = current_buffer.data.T segmentation = self.model(waveform[np.newaxis]).numpy()[0] # temporal resolution of the output of the model resolution = self.model.introspection.frames # temporal shift to keep track of current buffer start time resolution = SlidingWindow(start=current_buffer.sliding_window.start, duration=resolution.duration, step=resolution.step) # pyannote/segmentation pretrained model actually does more than just voice activity detection # see https://huggingface.co/pyannote/segmentation for more details. speech_probability = np.max(segmentation, axis=-1, keepdims=True) return SWF(speech_probability, resolution) . vad = VoiceActivityDetection() . Let us try this thing on current buffer: . current_buffer = next(buffer) current_buffer . vad(current_buffer) . reference . Building a basic streaming pipeline with streamz . We now have a way to stream audio and apply voice activity detection. According to its documentation, streamz seems like a good option to do that: . Streamz helps you build pipelines to manage continuous streams of data. . Let us start by creating a Stream that will ingest the rolling buffer and apply voice activity detection anytime the buffer is updated. . from streamz import Stream source = Stream() source.map(vad).sink(visualize) . We re-initialize the audio buffer from the start of the file and push the rolling buffer into the pipeline: . buffer = RollingAudioBuffer()(MY_AUDIO_FILE) source.emit(next(buffer)) . source.emit(next(buffer)) . source.emit(next(buffer)) . reference . Controlling latency / accuracy trade-off . This is nice but we can do better in case the pipeline is allowed a small delay (a.k.a. latency) between when it receives the audio and when it outputs the voice activity detection scores. . For instance, if we are allowed 2s latency, we could benefit from the multiple overlapping buffers and combine them to get a better estimate of the speech probability in regions where the model is not quite confident (e.g. just before t=4s). . This is what the Aggregation class does. . from typing import Tuple, List class Aggregation: &quot;&quot;&quot;Aggregate multiple overlapping buffers with a Parameters - latency : float, optional Allowed latency, in seconds. Defaults to 0. &quot;&quot;&quot; def __init__(self, latency=0.0): self.latency = latency def __call__(self, internal_state, current_buffer: SWF) -&gt; Tuple[Tuple[float, List[SWF]], SWF]: &quot;&quot;&quot;Ingest new buffer and return aggregated output with delay Parameters - internal_state : (internal_time, past_buffers) tuple `internal_time` is a float such that previous call emitted aggregated scores up to time `delayed_time`. `past_buffers` is a rolling list of past buffers that we are going to aggregate. current_buffer : SlidingWindowFeature New incoming score buffer. &quot;&quot;&quot; if internal_state is None: internal_state = (0.0, list()) # previous call led to the emission of aggregated scores up to time `delayed_time` # `past_buffers` is a rolling list of past buffers that we are going to aggregate delayed_time, past_buffers = internal_state # real time is the current end time of the audio buffer # (here, estimated from the end time of the VAD buffer) real_time = current_buffer.extent.end # because we are only allowed `self.latency` seconds of latency, this call should # return aggregated scores for [delayed_time, real_time - latency] time range. required = Segment(delayed_time, real_time - self.latency) # to compute more robust scores, we will combine all buffers that have a non-empty # temporal intersection with required time range. we can get rid of the others as they # will no longer be needed as they are too far away in the past. past_buffers = [buffer for buffer in past_buffers if buffer.extent.end &gt; required.start] + [current_buffer] # we aggregate all past buffers (but only on the &#39;required&#39; region of interest) intersection = np.stack([buffer.crop(required, fixed=required.duration) for buffer in past_buffers]) aggregation = np.mean(intersection, axis=0) # ... and wrap it into a self-contained SlidingWindowFeature (SWF) instance resolution = current_buffer.sliding_window resolution = SlidingWindow(start=required.start, duration=resolution.duration, step=resolution.step) output = SWF(aggregation, resolution) # we update the internal state delayed_time = real_time - self.latency internal_state = (delayed_time, past_buffers) # ... and return the whole thing for next call to know where we are return internal_state, output . Let&#39;s add this new accumulator into the streaming pipeline, with a 2s latency: . source = Stream() source .map(vad) .accumulate(Aggregation(latency=2.), returns_state=True, start=None) .sink(visualize) buffer = RollingAudioBuffer()(MY_AUDIO_FILE) . current_buffer = next(buffer); current_buffer . source.emit(current_buffer) . current_buffer = next(buffer); current_buffer . source.emit(current_buffer) . current_buffer = next(buffer); current_buffer . source.emit(current_buffer) . Look how the aggregation process actually refined the speech probability just before t=4s. This has been enabled by the longer latency. . That&#39;s all folks! . For technical questions and bug reports, please check pyannote.audio Github repository. . For commercial enquiries and scientific consulting, please contact me. . Bonus: concatenating output . For visualization purposes, you might want to add an accumulator to the pipeline that takes care of concatenating the output of each step... . class Concatenation: def __call__(self, concatenation: SWF, current_buffer: SWF) -&gt; Tuple[SWF, SWF]: if concatenation is None: return current_buffer, current_buffer resolution = concatenation.sliding_window current_start_frame = resolution.closest_frame(current_buffer.extent.start) current_end_frame = current_start_frame + len(current_buffer) concatenation.data = np.pad(concatenation.data, ((0, current_end_frame - len(concatenation.data)), (0, 0))) concatenation.data[current_start_frame: current_end_frame] = current_buffer.data return concatenation, concatenation . source = Stream() source .map(vad) .accumulate(Aggregation(latency=2.), returns_state=True, start=None) .accumulate(Concatenation(), returns_state=True, start=None) .sink(visualize) buffer = RollingAudioBuffer()(MY_AUDIO_FILE) . notebook.crop = Segment(0, 30) for _ in range(30): source.emit(next(buffer)) . reference .",
            "url": "https://herve.niderb.fr/fastpages/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html",
            "relUrl": "/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html",
            "date": " • Aug 5, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "Biography . I received my PhD on talking-face biometric authentication from Telecom ParisTech (France) in 2007. . In 2008, I joined Dublin City University (Ireland) to work on video summarization techniques applied to user-generated content. . I have been a permanent CNRS researcher since 2008: . at IRIT (Toulouse, France) in the SAMoVA group until 2010 | at LIMSI (Orsay, France) in the Spoken Language Processing group until 2020 | back at IRIT (Toulouse, France) until today | . My current research interests include deep learning techniques applied to machine listening, with a strong focus on speaker diarization: I am proud to be known as “the guy behind pyannote“. . Since 2021, I have also started working as a self-employed scientific advisor and am available for contracting on topics related to my academic research. . Contact . Postal Address: IRIT, 118 Route de Narbonne, F-31062 Toulouse, Cedex, France | Office: IRIT1 219 (2nd floor) | E-mail: herve.bredin@irit.fr | Tel: +33 (0)5 61 55 68 86 | .",
          "url": "https://herve.niderb.fr/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Collaborators",
          "content": "Current PhD students . Marvin Lavechin (2019 - today) . Unsupervised and multimodal protolexicon discovery | Co-supervised with Alejandrina Cristia (LSCP) and Emmanuel Dupoux (LSCP, FAIR) | . Juan Manuel Coria (2019 - today) . Active representation learning | Co-supervised with Sophie Rosset (LISN) and Sahar Ghannay (LISN) | . Former PhD students . Léo Galmant (2018 - 2020) . Machine learning for the automatic structuring of spoken conversations | Co-supervised with Anne-Laure Ligozat (LISN) and Camille Guinaudeau (LISN) | . Ruiqing Yin (2016-2019) . Steps towards end-to-end neural speaker diarization | Co-supervised with Claude Barras (LIMSI) | Now - Research scientist at Huawei | . Philipe Ercolessi (2010-2013) . Multimodal extraction of the narrative structure of TV series episodes | Co-supervised with Philippe Joly (IRIT) and Christine Sénac (IRIT) | Now - Head of research at Admo.tv | .",
          "url": "https://herve.niderb.fr/fastpages/collab/",
          "relUrl": "/collab/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "",
          "content": "Academic researcher . Deep learning for audio, speech and natural language processing . Tenured research scientist at French national institute for scientific research (CNRS) | Member of the Institut de Recherche en Informatique de Toulouse (IRIT) | Member of the SAMoVA team | . Publications: HAL or Google Scholar . Scientific advisor . Self-employed, available for contracting . Machine learning | Deep learning | Speech processing | Natural language processing | Machine listening | . Open source developer . Reproducible research advocate . Lead developer of pyannote.audio a deep learning toolkit for speaker diarization | Lead developer of pyannote.video face detection, tracking and clustering in videos | Lead developer of pyannote.metrics a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems | . All projects: Github . Blog .",
          "url": "https://herve.niderb.fr/fastpages/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://herve.niderb.fr/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}